{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topic Modelling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: gensim in /opt/anaconda3/lib/python3.8/site-packages (3.8.3)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /opt/anaconda3/lib/python3.8/site-packages (from gensim) (2.1.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: boto in /opt/anaconda3/lib/python3.8/site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in /opt/anaconda3/lib/python3.8/site-packages (from smart-open>=1.8.1->gensim) (1.15.2)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/anaconda3/lib/python3.8/site-packages (from smart-open>=1.8.1->gensim) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.19.0,>=1.18.2 in /opt/anaconda3/lib/python3.8/site-packages (from boto3->smart-open>=1.8.1->gensim) (1.18.2)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /opt/anaconda3/lib/python3.8/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /opt/anaconda3/lib/python3.8/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.8/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.9)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.8/site-packages (from requests->smart-open>=1.8.1->gensim) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/anaconda3/lib/python3.8/site-packages (from requests->smart-open>=1.8.1->gensim) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /opt/anaconda3/lib/python3.8/site-packages (from botocore<1.19.0,>=1.18.2->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/andreaprenner/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set WD\n",
    "os.chdir(\"/Users/andreaprenner/Desktop/Master_UvA/1. Semester/FundamentalsOfDS/Assignment1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_formatted = pd.read_pickle(\"twitter_data_classified_23k.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(w) for w in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   [insult, democracy, tweet, defeat]\n",
       "0    [morningjoe, alleged, child, rapist, lapdog, a...\n",
       "0                               [foley, sex, offender]\n",
       "0    [dont, come, admit, youre, bag, since, day, on...\n",
       "0              [clip, said, declare, victory, get, id]\n",
       "                           ...                        \n",
       "0      [thank, clarifying, hasnt, convicted, anything]\n",
       "0                                  [thanks, good, job]\n",
       "0    [scoop, n, gen, flynn, supporter, translater, ...\n",
       "0    [like, love, poorly, educated, supporter, igno...\n",
       "0    [half, supporter, basket, deplorables, crooked...\n",
       "Name: text_topicmodel, Length: 20846, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data cleaning - removing punctuation from text; still need to remove hashes\n",
    "data_formatted['text_topicmodel'] = data_formatted['text'].str.replace('@[^\\s]+','')\n",
    "data_formatted['text_topicmodel'] = data_formatted['text_topicmodel'].str.replace('[^\\w\\s]','')\n",
    "data_formatted['text_topicmodel'] = data_formatted['text_topicmodel'].str.replace('\\[.*?\\]','')\n",
    "data_formatted['text_topicmodel'] = data_formatted['text_topicmodel'].str.replace('[‘’“”…]','')\n",
    "data_formatted['text_topicmodel'] = data_formatted['text_topicmodel'].str.replace('\\w*\\d\\w*','')\n",
    "data_formatted['text_topicmodel'] = data_formatted['text_topicmodel'].apply(lambda x: x.lower())\n",
    "data_formatted['text_topicmodel'] = data_formatted['text_topicmodel'].str.replace(\"trump\", \"\")\n",
    "data_formatted['text_topicmodel'] = data_formatted['text_topicmodel'].str.replace(\"clinton\", \"\")\n",
    "data_formatted['text_topicmodel'] = data_formatted['text_topicmodel'].apply(lambda x: x.split())\n",
    "data_formatted['text_topicmodel'] = data_formatted['text_topicmodel'].apply(lambda x: [item for item in x if item not in stopwords])\n",
    "data_formatted['text_topicmodel'] = data_formatted['text_topicmodel'].apply(lemmatize_text)\n",
    "data_formatted['text_topicmodel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter out the rows where Trump and Clinton both are mentioned\n",
    "data_formatted = data_formatted[~((data_formatted['Trump'] == True) & (data_formatted['Clinton'] == True))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create data set for trump vs clinton\n",
    "trump_mentioned_positive = data_formatted[(data_formatted['Trump'] == True) & (data_formatted['classification_polarity'] == 'positive')]\n",
    "trump_mentioned_negative = data_formatted[(data_formatted['Trump'] == True) & (data_formatted['classification_polarity'] == 'negative')]\n",
    "clinton_mentioned_positive = data_formatted[(data_formatted['Clinton'] == True) & (data_formatted['classification_polarity'] == 'positive')]\n",
    "clinton_mentioned_negative = data_formatted[(data_formatted['Clinton'] == True) & (data_formatted['classification_polarity'] == 'negative')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trump mentioned / Positive Polarity Score / All words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.019*\"trump\" + 0.018*\"amp\" + 0.011*\"u\" + 0.008*\"nevertrump\" + 0.007*\"hillary\" + 0.006*\"love\" + 0.006*\"maga\" + 0.006*\"clinton\"'),\n",
       " (1,\n",
       "  '0.034*\"trump\" + 0.010*\"like\" + 0.008*\"u\" + 0.008*\"great\" + 0.008*\"america\" + 0.008*\"know\" + 0.007*\"one\" + 0.006*\"youre\"')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filter out rows where text doesnt contain any entries after removing tags and hashes\n",
    "trump_mentioned_positive = trump_mentioned_positive[trump_mentioned_positive['text_topicmodel'].map(lambda d: len(d)) > 0]\n",
    "trump_text = trump_mentioned_positive.text_topicmodel.tolist()\n",
    "dictionary_trump = corpora.Dictionary(trump_text)\n",
    "doc_term_matrix_trump = [dictionary_trump.doc2bow(doc) for doc in trump_text]\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda_trump = gensim.models.ldamodel.LdaModel\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel_trump = Lda_trump(doc_term_matrix_trump, num_topics=2, id2word = dictionary_trump, passes=100)\n",
    "topics_trump = ldamodel_trump.print_topics(num_topics=2, num_words=8)\n",
    "topics_trump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trump mentioned / Negative Polarity Score / All words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.033*\"trump\" + 0.010*\"today\" + 0.009*\"pressure\" + 0.009*\"rain\" + 0.009*\"tempcrab\" + 0.009*\"forecast\" + 0.009*\"orchard\" + 0.007*\"fine\"'),\n",
       " (1,\n",
       "  '0.020*\"trump\" + 0.015*\"u\" + 0.010*\"amp\" + 0.009*\"dont\" + 0.008*\"get\" + 0.008*\"like\" + 0.008*\"people\" + 0.008*\"hillary\"')]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_mentioned_negative = trump_mentioned_negative[trump_mentioned_negative['text_topicmodel'].map(lambda d: len(d)) > 0]\n",
    "trump_text = trump_mentioned_negative.text_topicmodel.tolist()\n",
    "dictionary_trump = corpora.Dictionary(trump_text)\n",
    "doc_term_matrix_trump = [dictionary_trump.doc2bow(doc) for doc in trump_text]\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda_trump = gensim.models.ldamodel.LdaModel\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel_trump = Lda_trump(doc_term_matrix_trump, num_topics=2, id2word = dictionary_trump, passes=100)\n",
    "topics_trump = ldamodel_trump.print_topics(num_topics=2, num_words=8)\n",
    "topics_trump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clinton mentioned / Positive Polarity Score / All words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.015*\"imwithher\" + 0.015*\"hillary\" + 0.014*\"trump\" + 0.012*\"amp\"'),\n",
       " (1, '0.011*\"trump\" + 0.010*\"amp\" + 0.009*\"u\" + 0.008*\"hillary\"')]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filter out rows where text doesnt contain any entries after removing tags and hashes\n",
    "clinton_mentioned_positive = clinton_mentioned_positive[clinton_mentioned_positive['text_topicmodel'].map(lambda d: len(d)) > 0]\n",
    "clinton_text = clinton_mentioned_positive.text_topicmodel.tolist()\n",
    "dictionary_clinton = corpora.Dictionary(clinton_text)\n",
    "doc_term_matrix_clinton = [dictionary_clinton.doc2bow(doc) for doc in clinton_text]\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda_clinton = gensim.models.ldamodel.LdaModel\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel_clinton = Lda_clinton(doc_term_matrix_clinton, num_topics=2, id2word = dictionary_clinton, passes=100)\n",
    "topics_clinton = ldamodel_clinton.print_topics(num_topics=10, num_words=4)\n",
    "topics_clinton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clinton mentioned / Negative Polarity Score / All words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.013*\"amp\" + 0.012*\"hillary\" + 0.012*\"u\" + 0.011*\"trump\"'),\n",
       " (1, '0.015*\"trump\" + 0.013*\"imwithher\" + 0.011*\"hillary\" + 0.011*\"clinton\"')]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filter out rows where text doesnt contain any entries after removing tags and hashes\n",
    "clinton_mentioned_negative = clinton_mentioned_negative[clinton_mentioned_negative['text_topicmodel'].map(lambda d: len(d)) > 0]\n",
    "clinton_text = clinton_mentioned_positive.text_topicmodel.tolist()\n",
    "dictionary_clinton = corpora.Dictionary(clinton_text)\n",
    "doc_term_matrix_clinton = [dictionary_clinton.doc2bow(doc) for doc in clinton_text]\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda_clinton = gensim.models.ldamodel.LdaModel\n",
    "# Running and Training LDA model on the document term matrix.\n",
    "ldamodel_clinton = Lda_clinton(doc_term_matrix_clinton, num_topics=2, id2word = dictionary_clinton, passes=100)\n",
    "topics_clinton = ldamodel_clinton.print_topics(num_topics=10, num_words=4)\n",
    "topics_clinton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Just using nouns in Topic Modelling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "from nltk import word_tokenize, pos_tag\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trump mentioned / Positive Polarity Score / Only Nouns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [(insult, NN), (democracy, NN), (tweet, NN), (...\n",
       "0    [(morningjoe, NNS), (alleged, VBD), (child, JJ...\n",
       "0    [(dont, JJ), (come, JJ), (admit, NN), (youre, ...\n",
       "0    [(clip, NN), (said, VBD), (declare, JJ), (vict...\n",
       "0    [(word, NN), (consequence, NN), (loser, NN), (...\n",
       "                           ...                        \n",
       "0    [(remember, VB), (slick, JJ), (willie, NN), (s...\n",
       "0    [(somebody, NN), (smoking, VBG), (something, N...\n",
       "0    [(think, VB), (snort, NN), (coke, NNS), (take,...\n",
       "0               [(thanks, NNS), (good, JJ), (job, NN)]\n",
       "0    [(like, IN), (love, JJS), (poorly, RB), (educa...\n",
       "Name: text_topicmodel, Length: 7681, dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Apply function that determines the type of each word\n",
    "trump_mentioned_text = trump_mentioned_positive['text_topicmodel'].apply(lambda x: nltk.pos_tag(x))\n",
    "trump_mentioned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_noun = lambda pos: pos[:2] == 'NN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter out the nouns for each row\n",
    "trump_mentioned_nouns = pd.DataFrame(np.zeros((len(trump_mentioned_text), 1)))\n",
    "trump_mentioned_nouns.columns = ['Text']\n",
    "for i in range(0, len(trump_mentioned_text)):\n",
    "    nouns = [word for (word, pos) in trump_mentioned_text.iloc[i] if is_noun(pos)]\n",
    "    nouns = ' '.join(nouns)\n",
    "    trump_mentioned_nouns.iloc[i] = nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the Text \n",
    "trump_mentioned_nouns['Text'] = trump_mentioned_nouns['Text'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.039*\"job\" + 0.029*\"president\" + 0.027*\"word\" + 0.025*\"putin\" + 0.019*\"everyone\"'),\n",
       " (1,\n",
       "  '0.031*\"time\" + 0.027*\"wall\" + 0.024*\"idiot\" + 0.019*\"day\" + 0.016*\"dump\"'),\n",
       " (2,\n",
       "  '0.029*\"way\" + 0.026*\"truth\" + 0.020*\"man\" + 0.019*\"candidate\" + 0.017*\"train\"'),\n",
       " (3,\n",
       "  '0.033*\"vote\" + 0.027*\"love\" + 0.024*\"guy\" + 0.022*\"thank\" + 0.020*\"campaign\"'),\n",
       " (4,\n",
       "  '0.053*\"america\" + 0.027*\"im\" + 0.018*\"maga\" + 0.013*\"time\" + 0.013*\"amp\"'),\n",
       " (5,\n",
       "  '0.042*\"donald\" + 0.030*\"youre\" + 0.025*\"r\" + 0.019*\"maga\" + 0.019*\"obama\"'),\n",
       " (6,\n",
       "  '0.028*\"country\" + 0.022*\"poll\" + 0.017*\"pay\" + 0.015*\"idea\" + 0.014*\"shes\"'),\n",
       " (7,\n",
       "  '0.053*\"tax\" + 0.029*\"plan\" + 0.028*\"look\" + 0.024*\"return\" + 0.019*\"release\"'),\n",
       " (8,\n",
       "  '0.054*\"people\" + 0.025*\"supporter\" + 0.024*\"racist\" + 0.019*\"amp\" + 0.018*\"support\"'),\n",
       " (9,\n",
       "  '0.024*\"mexico\" + 0.022*\"lie\" + 0.021*\"leader\" + 0.021*\"win\" + 0.019*\"woman\"')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_text = trump_mentioned_nouns.Text.tolist()\n",
    "dictionary_trump = corpora.Dictionary(trump_text)\n",
    "doc_term_matrix_trump = [dictionary_trump.doc2bow(doc) for doc in trump_text]\n",
    "Lda_trump = gensim.models.ldamodel.LdaModel\n",
    "ldamodel_trump_1 = Lda_trump(doc_term_matrix_trump, num_topics=10, id2word = dictionary_trump, passes=2)\n",
    "ldamodel_trump_1.print_topics(num_topics=10, num_words=5)\n",
    "#for idx, topic in ldamodel_trump.print_topics(-1):\n",
    " #   print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trump mentioned / Negative Polarity Score / Only Nouns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [(msnbc, NN), (cnn, NN), (mention, NN), (fox, ...\n",
       "0    [(trueanother, RB), (fasle, JJ), (coverage, NN...\n",
       "0    [(didnt, NN), (sound, NN), (sarcastic, JJ), (i...\n",
       "0    [(isi, NN), (trouble, NN), (flip, NN), (get, N...\n",
       "0    [(trying, VBG), (lose, JJ), (cant, JJ), (wait,...\n",
       "                           ...                        \n",
       "0    [(manny, JJ), (alotta, NN), (people, NNS), (sa...\n",
       "0    [(last, JJ), (day, NN), (strong, JJ), (man, NN...\n",
       "0    [(joining, VBG), (administration, NN), (rumore...\n",
       "0    [(mean, JJ), (speech, NN), (home, NN), (run, N...\n",
       "0    [(totem, NN), (pole, JJ), (little, JJ), (bill,...\n",
       "Name: text_topicmodel, Length: 5636, dtype: object"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Apply function that determines the type of each word\n",
    "trump_mentioned_text = trump_mentioned_negative['text_topicmodel'].apply(lambda x: nltk.pos_tag(x))\n",
    "trump_mentioned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter out the nouns for each row\n",
    "trump_mentioned_nouns = pd.DataFrame(np.zeros((len(trump_mentioned_text), 1)))\n",
    "trump_mentioned_nouns.columns = ['Text']\n",
    "for i in range(0, len(trump_mentioned_text)):\n",
    "    nouns = [word for (word, pos) in trump_mentioned_text.iloc[i] if is_noun(pos)]\n",
    "    nouns = ' '.join(nouns)\n",
    "    trump_mentioned_nouns.iloc[i] = nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the Text \n",
    "trump_mentioned_nouns['Text'] = trump_mentioned_nouns['Text'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.030*\"president\" + 0.021*\"work\" + 0.020*\"racist\" + 0.017*\"need\" + 0.015*\"debate\"'),\n",
       " (1,\n",
       "  '0.027*\"speech\" + 0.027*\"immigration\" + 0.018*\"dont\" + 0.015*\"day\" + 0.013*\"donald\"'),\n",
       " (2,\n",
       "  '0.032*\"lie\" + 0.022*\"didnt\" + 0.016*\"amp\" + 0.015*\"state\" + 0.014*\"mexico\"'),\n",
       " (3,\n",
       "  '0.045*\"maga\" + 0.019*\"world\" + 0.016*\"poll\" + 0.015*\"shit\" + 0.014*\"show\"'),\n",
       " (4,\n",
       "  '0.018*\"putin\" + 0.018*\"thing\" + 0.014*\"news\" + 0.013*\"think\" + 0.013*\"anyone\"'),\n",
       " (5,\n",
       "  '0.025*\"amp\" + 0.020*\"wall\" + 0.020*\"get\" + 0.020*\"medium\" + 0.013*\"hillary\"'),\n",
       " (6,\n",
       "  '0.025*\"hate\" + 0.024*\"doesnt\" + 0.023*\"anything\" + 0.017*\"thats\" + 0.015*\"people\"'),\n",
       " (7,\n",
       "  '0.038*\"donald\" + 0.029*\"vote\" + 0.027*\"year\" + 0.025*\"talk\" + 0.020*\"im\"'),\n",
       " (8,\n",
       "  '0.021*\"today\" + 0.018*\"plan\" + 0.017*\"amp\" + 0.014*\"candidate\" + 0.014*\"something\"'),\n",
       " (9,\n",
       "  '0.033*\"nothing\" + 0.033*\"people\" + 0.029*\"tax\" + 0.016*\"voter\" + 0.015*\"country\"')]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_text = trump_mentioned_nouns.Text.tolist()\n",
    "dictionary_trump = corpora.Dictionary(trump_text)\n",
    "doc_term_matrix_trump = [dictionary_trump.doc2bow(doc) for doc in trump_text]\n",
    "ldamodel_trump_2 = Lda_trump(doc_term_matrix_trump, num_topics=10, id2word = dictionary_trump, passes=2)\n",
    "ldamodel_trump_2.print_topics(num_topics=10, num_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clinton mentioned / Positive Polarity Score / Only Nouns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             [(foley, NN), (sex, NN), (offender, NN)]\n",
       "0    [(press, NN), (advocate, NN), (lame, JJ), (duc...\n",
       "0          [(yes, RB), (exactly, RB), (imwithher, RB)]\n",
       "0    [(attacking, VBG), (gave, VBD), (pas, JJ), (pa...\n",
       "0    [(american, JJ), (people, NNS), (right, RB), (...\n",
       "                           ...                        \n",
       "0    [(unfit, JJ), (medically, RB), (become, VBN), ...\n",
       "0    [(totally, RB), (different, JJ), (statute, NN)...\n",
       "0    [(real, JJ), (hillary, JJ), (every, DT), (time...\n",
       "0                         [(aug, NN), (imwithher, NN)]\n",
       "0    [(copy, NN), (speech, NN), (made, VBD), (print...\n",
       "Name: text_topicmodel, Length: 2889, dtype: object"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Apply function that determines the type of each word\n",
    "clinton_mentioned_text = clinton_mentioned_positive['text_topicmodel'].apply(lambda x: nltk.pos_tag(x))\n",
    "clinton_mentioned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter out the nouns for each row\n",
    "clinton_mentioned_nouns = pd.DataFrame(np.zeros((len(clinton_mentioned_text), 1)))\n",
    "clinton_mentioned_nouns.columns = ['Text']\n",
    "for i in range(0, len(clinton_mentioned_text)):\n",
    "    nouns = [word for (word, pos) in clinton_mentioned_text.iloc[i] if is_noun(pos)]\n",
    "    nouns = ' '.join(nouns)\n",
    "    clinton_mentioned_nouns.iloc[i] = nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the Text \n",
    "clinton_mentioned_nouns['Text'] = clinton_mentioned_nouns['Text'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.028*\"people\" + 0.020*\"email\" + 0.018*\"vote\" + 0.016*\"plan\" + 0.012*\"obama\"'),\n",
       " (1,\n",
       "  '0.024*\"time\" + 0.020*\"press\" + 0.019*\"people\" + 0.018*\"lie\" + 0.017*\"medium\"'),\n",
       " (2,\n",
       "  '0.026*\"hillary\" + 0.024*\"day\" + 0.013*\"corrupt\" + 0.012*\"video\" + 0.009*\"election\"'),\n",
       " (3,\n",
       "  '0.020*\"way\" + 0.019*\"president\" + 0.017*\"dont\" + 0.016*\"love\" + 0.015*\"time\"'),\n",
       " (4,\n",
       "  '0.051*\"amp\" + 0.031*\"r\" + 0.028*\"imwithher\" + 0.019*\"u\" + 0.016*\"supporter\"'),\n",
       " (5,\n",
       "  '0.021*\"jail\" + 0.016*\"america\" + 0.015*\"campaign\" + 0.011*\"cant\" + 0.010*\"fund\"'),\n",
       " (6,\n",
       "  '0.017*\"thats\" + 0.014*\"health\" + 0.012*\"show\" + 0.012*\"r\" + 0.011*\"donald\"'),\n",
       " (7,\n",
       "  '0.019*\"foundation\" + 0.012*\"pay\" + 0.010*\"vet\" + 0.009*\"time\" + 0.009*\"amp\"'),\n",
       " (8,\n",
       "  '0.071*\"imwithher\" + 0.015*\"liar\" + 0.013*\"woman\" + 0.010*\"crookedhillary\" + 0.010*\"isi\"'),\n",
       " (9,\n",
       "  '0.029*\"shes\" + 0.025*\"question\" + 0.023*\"amp\" + 0.015*\"hell\" + 0.013*\"war\"')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clinton_text = clinton_mentioned_nouns.Text.tolist()\n",
    "dictionary_clinton = corpora.Dictionary(clinton_text)\n",
    "doc_term_matrix_clinton = [dictionary_clinton.doc2bow(doc) for doc in clinton_text]\n",
    "Lda_clinton = gensim.models.ldamodel.LdaModel\n",
    "ldamodel_clinton_1 = Lda_clinton(doc_term_matrix_clinton, num_topics=10, id2word = dictionary_clinton, passes=2)\n",
    "ldamodel_clinton_1.print_topics(num_topics=10, num_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clinton mentioned / Negative Polarity Score / Only Nouns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [(cant, JJ), (one, CD), (til, NN), (read, VBD)...\n",
       "0    [(clearly, RB), (top, JJ), (secret, JJ), (info...\n",
       "0    [(randpaul, NN), (lying, VBG), (congress, NN),...\n",
       "0    [(randpaul, NN), (lying, VBG), (congress, NN),...\n",
       "0    [(randpaul, NN), (lying, VBG), (congress, NN),...\n",
       "                           ...                        \n",
       "0    [(always, RB), (angry, JJ), (dont, NN), (like,...\n",
       "0    [(hillary, JJ), (get, NN), (focused, VBD), (me...\n",
       "0    [(please, NN), (stand, VB), (standingrocksioux...\n",
       "0    [(thank, NN), (clarifying, VBG), (hasnt, NN), ...\n",
       "0    [(half, NN), (supporter, NN), (basket, NN), (d...\n",
       "Name: text_topicmodel, Length: 2158, dtype: object"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Apply function that determines the type of each word\n",
    "clinton_mentioned_text = clinton_mentioned_negative['text_topicmodel'].apply(lambda x: nltk.pos_tag(x))\n",
    "clinton_mentioned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter out the nouns for each row\n",
    "clinton_mentioned_nouns = pd.DataFrame(np.zeros((len(clinton_mentioned_text), 1)))\n",
    "clinton_mentioned_nouns.columns = ['Text']\n",
    "for i in range(0, len(clinton_mentioned_text)):\n",
    "    nouns = [word for (word, pos) in clinton_mentioned_text.iloc[i] if is_noun(pos)]\n",
    "    nouns = ' '.join(nouns)\n",
    "    clinton_mentioned_nouns.iloc[i] = nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the Text \n",
    "clinton_mentioned_nouns['Text'] = clinton_mentioned_nouns['Text'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.041*\"lie\" + 0.025*\"didnt\" + 0.023*\"ground\" + 0.022*\"troop\" + 0.019*\"medium\"'),\n",
       " (1,\n",
       "  '0.028*\"r\" + 0.023*\"money\" + 0.014*\"something\" + 0.014*\"dont\" + 0.013*\"hillary\"'),\n",
       " (2,\n",
       "  '0.024*\"vote\" + 0.022*\"dont\" + 0.019*\"plan\" + 0.018*\"shes\" + 0.017*\"cold\"'),\n",
       " (3,\n",
       "  '0.023*\"hill\" + 0.022*\"imwithher\" + 0.021*\"fact\" + 0.019*\"people\" + 0.016*\"evil\"'),\n",
       " (4,\n",
       "  '0.036*\"year\" + 0.030*\"education\" + 0.025*\"press\" + 0.023*\"conference\" + 0.013*\"death\"'),\n",
       " (5,\n",
       "  '0.032*\"liar\" + 0.030*\"get\" + 0.023*\"people\" + 0.019*\"control\" + 0.016*\"show\"'),\n",
       " (6,\n",
       "  '0.032*\"time\" + 0.024*\"men\" + 0.019*\"state\" + 0.018*\"america\" + 0.016*\"cause\"'),\n",
       " (7,\n",
       "  '0.026*\"campaign\" + 0.026*\"office\" + 0.022*\"obama\" + 0.020*\"amp\" + 0.020*\"state\"'),\n",
       " (8,\n",
       "  '0.035*\"dont\" + 0.031*\"people\" + 0.027*\"man\" + 0.019*\"anything\" + 0.018*\"speech\"'),\n",
       " (9,\n",
       "  '0.034*\"reason\" + 0.028*\"friend\" + 0.024*\"job\" + 0.023*\"corruption\" + 0.022*\"youre\"')]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clinton_text = clinton_mentioned_nouns.Text.tolist()\n",
    "dictionary_clinton = corpora.Dictionary(clinton_text)\n",
    "doc_term_matrix_clinton = [dictionary_clinton.doc2bow(doc) for doc in clinton_text]\n",
    "ldamodel_clinton_2 = Lda_clinton(doc_term_matrix_clinton, num_topics=10, id2word = dictionary_clinton, passes=2)\n",
    "ldamodel_clinton_2.print_topics(num_topics=10, num_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_table_trump_po = pd.DataFrame(np.zeros((5, 10)), \n",
    "                            index = ['Trump - Pos', 'Trump - Pos', 'Trump - Pos', 'Trump - Pos', 'Trump - Pos'])\n",
    "topics_table_trump_po.columns = ['Topic1', 'Topic2', 'Topic3', 'Topic4', 'Topic5', 'Topic6', 'Topic7', 'Topic8', 'Topic9', 'Topic10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    for z in range(0,5):\n",
    "        topics_table_trump_po.iloc[z,i] = ldamodel_trump_1.print_topics(num_topics=10, num_words=5)[i][1].split(\"+\")[z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_table_trump_neg = pd.DataFrame(np.zeros((5, 10)), \n",
    "                            index = ['Trump - Neg', 'Trump - Neg', 'Trump - Neg', 'Trump - Neg', 'Trump - Neg'])\n",
    "topics_table_trump_neg.columns = ['Topic1', 'Topic2', 'Topic3', 'Topic4', 'Topic5', 'Topic6', 'Topic7', 'Topic8', 'Topic9', 'Topic10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    for z in range(0,5):\n",
    "        topics_table_trump_neg.iloc[z,i] = ldamodel_trump_2.print_topics(num_topics=10, num_words=5)[i][1].split(\"+\")[z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_table_clinton_po = pd.DataFrame(np.zeros((5, 10)), \n",
    "                            index = ['Clinton - Pos', 'Clinton - Pos', 'Clinton - Pos', 'Clinton - Pos', 'Clinton - Pos'])\n",
    "topics_table_clinton_po.columns = ['Topic1', 'Topic2', 'Topic3', 'Topic4', 'Topic5', 'Topic6', 'Topic7', 'Topic8', 'Topic9', 'Topic10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    for z in range(0,5):\n",
    "        topics_table_clinton_po.iloc[z,i] = ldamodel_clinton_1.print_topics(num_topics=10, num_words=5)[i][1].split(\"+\")[z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_table_clinton_neg = pd.DataFrame(np.zeros((5, 10)), \n",
    "                            index = ['Clinton - Neg', 'Clinton - Neg', 'Clinton - Neg', 'Clinton - Neg', 'Clinton - Neg'])\n",
    "topics_table_clinton_neg.columns = ['Topic1', 'Topic2', 'Topic3', 'Topic4', 'Topic5', 'Topic6', 'Topic7', 'Topic8', 'Topic9', 'Topic10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    for z in range(0,5):\n",
    "        topics_table_clinton_neg.iloc[z,i] = ldamodel_clinton_2.print_topics(num_topics=10, num_words=5)[i][1].split(\"+\")[z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_topicmodel = pd.concat([topics_table_trump_po, topics_table_trump_neg, topics_table_clinton_po, topics_table_clinton_neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_topicmodel.to_csv('output_topicmodel.csv', sep = \",\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing one tweet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test specific document how it would have been classified \n",
    "bow_doc_1 = doc_term_matrix_trump[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 39 (\"day\") appears 1 time.\n",
      "Word 40 (\"release\") appears 1 time.\n",
      "Word 41 (\"sorry\") appears 1 time.\n",
      "Word 42 (\"weapon\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(bow_doc_1)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_1[i][0], \n",
    "                                               dictionary_trump[bow_doc_1[i][0]], \n",
    "bow_doc_1[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text                @sethmoulton @realDonaldTrump will release nuc...\n",
       "user_screen_name                                             karmakar\n",
       "created_at                             Fri Aug 12 11:24:23 +0000 2016\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_mentioned_negative.iloc[10][['text', 'user_screen_name', 'created_at']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@sethmoulton @realDonaldTrump will release nuclear weapons and the next day say 'sorry I was being sarcastic'\""
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_mentioned_negative.iloc[10]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-123-de9db2a9efcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldamodel_trump_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbow_doc_1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nScore: {}\\t \\nTopic: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mldamodel_trump\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_topic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/basemodel.py\u001b[0m in \u001b[0;36mprint_topic\u001b[0;34m(self, topicno, topn)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \"\"\"\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m' + '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%.3f*\"%s\"'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_topic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopicno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mshow_topic\u001b[0;34m(self, topicid, topn)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \"\"\"\n\u001b[0;32m-> 1192\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_topic_terms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopicid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mget_topic_terms\u001b[0;34m(self, topicid, topn)\u001b[0m\n\u001b[1;32m   1222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m         \"\"\"\n\u001b[0;32m-> 1224\u001b[0;31m         \u001b[0mtopic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopicid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1225\u001b[0m         \u001b[0mtopic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# normalize to probability distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m         \u001b[0mbestn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 4 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(ldamodel_trump_2[bow_doc_1], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, ldamodel_trump.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic1</th>\n",
       "      <th>Topic2</th>\n",
       "      <th>Topic3</th>\n",
       "      <th>Topic4</th>\n",
       "      <th>Topic5</th>\n",
       "      <th>Topic6</th>\n",
       "      <th>Topic7</th>\n",
       "      <th>Topic8</th>\n",
       "      <th>Topic9</th>\n",
       "      <th>Topic10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic1  Topic2  Topic3  Topic4  Topic5  Topic6  Topic7  Topic8  Topic9  \\\n",
       "0      0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1      0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2      0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3      0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4      0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "5      0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "6      0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "7      0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "8      0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9      0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "10     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "    Topic10  \n",
       "0       0.0  \n",
       "1       0.0  \n",
       "2       0.0  \n",
       "3       0.0  \n",
       "4       0.0  \n",
       "5       0.0  \n",
       "6       0.0  \n",
       "7       0.0  \n",
       "8       0.0  \n",
       "9       0.0  \n",
       "10      0.0  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_table = pd.DataFrame(np.zeros((11, 10)))\n",
    "topics_table.columns = ['Topic1', 'Topic2', 'Topic3', 'Topic4', 'Topic5', 'Topic6', 'Topic7', 'Topic8', 'Topic9', 'Topic10']\n",
    "topics_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 10):\n",
    "    for z in range(0,10):\n",
    "        topics_table.iloc[z,i] = (ldamodel_trump_2.print_topic(i, 10)).split(\"+\")[z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, score in sorted(ldamodel_trump_2[bow_doc_1], key=lambda tup: -1*tup[1]):\n",
    "    for i in range(0,10): \n",
    "        topics_table.iloc[10,i] = ldamodel_trump_2[bow_doc_1][i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_table.to_csv('topics_table.csv', sep = \",\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
