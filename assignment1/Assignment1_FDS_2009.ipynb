{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A1h0k7ifCmwL"
   },
   "source": [
    "# Libraries, Packages and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "colab_type": "code",
    "id": "3a-ZqEFACKEV",
    "outputId": "7e5e769c-3388-4088-e748-ae32506f39c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geocoder\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4f/6b/13166c909ad2f2d76b929a4227c952630ebaf0d729f6317eb09cbceccbab/geocoder-1.38.1-py2.py3-none-any.whl (98kB)\n",
      "\u001b[K    100% |████████████████████████████████| 102kB 2.8MB/s a 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: future in /anaconda3/lib/python3.7/site-packages (from geocoder) (0.18.2)\n",
      "Requirement already satisfied: click in /anaconda3/lib/python3.7/site-packages (from geocoder) (7.0)\n",
      "Collecting ratelim (from geocoder)\n",
      "  Downloading https://files.pythonhosted.org/packages/f2/98/7e6d147fd16a10a5f821db6e25f192265d6ecca3d82957a4fdd592cad49c/ratelim-0.1.6-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six in /anaconda3/lib/python3.7/site-packages (from geocoder) (1.12.0)\n",
      "Requirement already satisfied: requests in /anaconda3/lib/python3.7/site-packages (from geocoder) (2.21.0)\n",
      "Requirement already satisfied: decorator in /anaconda3/lib/python3.7/site-packages (from ratelim->geocoder) (4.3.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda3/lib/python3.7/site-packages (from requests->geocoder) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/lib/python3.7/site-packages (from requests->geocoder) (2018.11.29)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /anaconda3/lib/python3.7/site-packages (from requests->geocoder) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /anaconda3/lib/python3.7/site-packages (from requests->geocoder) (1.24.1)\n",
      "Installing collected packages: ratelim, geocoder\n",
      "Successfully installed geocoder-1.38.1 ratelim-0.1.6\n"
     ]
    }
   ],
   "source": [
    "!pip install geocoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geopy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/e1/9c72de674d5c2b8fcb0738a5ceeb5424941fefa080bfe4e240d0bacb5a38/geopy-2.0.0-py3-none-any.whl (111kB)\n",
      "\u001b[K    100% |████████████████████████████████| 112kB 2.4MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting geographiclib<2,>=1.49 (from geopy)\n",
      "  Downloading https://files.pythonhosted.org/packages/8b/62/26ec95a98ba64299163199e95ad1b0e34ad3f4e176e221c40245f211e425/geographiclib-1.50-py3-none-any.whl\n",
      "Installing collected packages: geographiclib, geopy\n",
      "Successfully installed geographiclib-1.50 geopy-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P0iNXAxiZ14S"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from google.colab import drive\n",
    "from geopy import Nominatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FbsHS26gDGvQ"
   },
   "outputs": [],
   "source": [
    "geolocator = Nominatim(user_agent=\"geoapiExercises\")\n",
    "\n",
    "def city_state_country(coord):\n",
    "  try:\n",
    "    location = geolocator.reverse(coord, exactly_one=True)\n",
    "    address = location.raw['address']\n",
    "    #city = address.get('city', '')\n",
    "    state = address.get('state', '')\n",
    "    #country = address.get('country', '')\n",
    "    return state\n",
    "  except:\n",
    "    return 'NULL'\n",
    "#print(city_state_country(\"47.470706, -99.704723\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MIC1RnXWCsob"
   },
   "source": [
    "# 1) Loading source File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "id": "qkk8l788Z14Z",
    "outputId": "928cb1a9-7baf-4141-eacd-311d13e146a0"
   },
   "outputs": [],
   "source": [
    "#Set WD\n",
    "# Use this line in Colab\n",
    "#drive.mount(\"/content/drive\")\n",
    "#Use this line in local jupyter\n",
    "os.chdir(\"/Users/andreaprenner/Desktop/Master_UvA/1. Semester/FundamentalsOfDS/Assignment1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "id": "9gbcqi0kZ14d",
    "outputId": "d7e6f9cb-506a-4e7d-cc59-42cfc23993a3"
   },
   "outputs": [],
   "source": [
    "# Use this line in Colab\n",
    "#tar = tarfile.open('/content/drive/My Drive/Colab Notebooks/Assignment 1 FDS/geotagged_tweets_20160812-0912.tar')\n",
    "# Use this line in local jupyter\n",
    "tar = tarfile.open(\"geotagged_tweets_20160812-0912.tar.gz\")\n",
    "files = tar.getmembers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PRDjL5XwZ14g"
   },
   "outputs": [],
   "source": [
    "f = tar.extractfile(files[0]) \n",
    "data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uj5qtIaKZ14i"
   },
   "outputs": [],
   "source": [
    "#One tweet entry in raw data file\n",
    "json.loads(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rthrTidiZyOB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "657307"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dUzsYxB9IWrH"
   },
   "outputs": [],
   "source": [
    "for i in range(0, 10000): \n",
    "  dic = json.loads(data[i*6]) #Why i times 6?\n",
    "  \n",
    "  try:\n",
    "    # first iteration we create the dataframe\n",
    "    if i == 0:\n",
    "      base_data = {'created_at': [dic['created_at']]\n",
    "                  ,'text': [dic['text']]\n",
    "                  ,'id': [dic['id']]\n",
    "                  ,'tweet_lang': [dic['lang']]\n",
    "                  ,'in_reply_to_user_id': [dic['in_reply_to_user_id']]\n",
    "                  ,'in_reply_to_user_id_str': [dic['in_reply_to_user_id_str']]\n",
    "                  ,'user_id': [dic['user']['id']]\n",
    "                  ,'user_screen_name' : [dic['user']['screen_name']]\n",
    "                  ,'user_location': [dic['user']['location']]\n",
    "                  ,'user_description': [dic['user']['description']]\n",
    "                  ,'user_verified': [dic['user']['verified']]\n",
    "                  ,'user_followers_count': [dic['user']['followers_count']]\n",
    "                  ,'user_friends_count': [dic['user']['friends_count']]\n",
    "                  ,'user_statuses_count': [dic['user']['statuses_count']]\n",
    "                  ,'user_created_at': [dic['user']['created_at']]\n",
    "                  ,'user_lang': [dic['user']['lang']]\n",
    "                  #,'place_id' : [dic['place']['id']]\n",
    "                  ,'place_name' : [dic['place']['name']]\n",
    "                  ,'place_full_name' : [dic['place']['full_name']]\n",
    "                  ,'place_country_code' : [dic['place']['country_code']]\n",
    "                  ,'place_country' : [dic['place']['country']]\n",
    "                  ,'place_latitude' : [dic['place']['bounding_box']['coordinates'][0][0][0]]\n",
    "                  ,'place_longitude' : [dic['place']['bounding_box']['coordinates'][0][0][1]]\n",
    "                  ,'entities_hashtags' : [dic['entities']['hashtags']]\n",
    "                  ,'entities_user_mentions' : [dic['entities']['user_mentions']]\n",
    "                  }\n",
    "      df_base = pd.DataFrame(data=base_data)\n",
    "    \n",
    "    # next iterations we append data\n",
    "    else:\n",
    "      next_data = {'created_at': [dic['created_at']]\n",
    "                  ,'text': [dic['text']]\n",
    "                  ,'id': [dic['id']]\n",
    "                  ,'tweet_lang': [dic['lang']]\n",
    "                  ,'in_reply_to_user_id': [dic['in_reply_to_user_id']]\n",
    "                  ,'in_reply_to_user_id_str': [dic['in_reply_to_user_id_str']]\n",
    "                  ,'user_id': [dic['user']['id']]\n",
    "                  ,'user_screen_name' : [dic['user']['screen_name']]\n",
    "                  ,'user_location': [dic['user']['location']]\n",
    "                  ,'user_description': [dic['user']['description']]\n",
    "                  ,'user_verified': [dic['user']['verified']]\n",
    "                  ,'user_followers_count': [dic['user']['followers_count']]\n",
    "                  ,'user_friends_count': [dic['user']['friends_count']]\n",
    "                  ,'user_statuses_count': [dic['user']['statuses_count']]\n",
    "                  ,'user_created_at': [dic['user']['created_at']]\n",
    "                  ,'user_lang': [dic['user']['lang']]\n",
    "                  #,'place_id' : [dic['place']['id']]\n",
    "                  ,'place_name' : [dic['place']['name']]\n",
    "                  ,'place_full_name' : [dic['place']['full_name']]\n",
    "                  ,'place_country_code' : [dic['place']['country_code']]\n",
    "                  ,'place_country' : [dic['place']['country']]\n",
    "                  ,'place_latitude' : [dic['place']['bounding_box']['coordinates'][0][0][0]]\n",
    "                  ,'place_longitude' : [dic['place']['bounding_box']['coordinates'][0][0][1]]\n",
    "                  ,'entities_hashtags' : [dic['entities']['hashtags']]\n",
    "                  ,'entities_user_mentions' : [dic['entities']['user_mentions']]\n",
    "                  }            \n",
    "      df_aux = pd.DataFrame(data=next_data)\n",
    "      df_base = df_base.append(df_aux)\n",
    "  except:\n",
    "    print(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GPpwE2uS11iG"
   },
   "outputs": [],
   "source": [
    "data_formatted = df_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xjj2qhBjbhKg"
   },
   "outputs": [],
   "source": [
    "# Save Data\n",
    "#data_formatted.to_pickle('/content/drive/My Drive/Colab Notebooks/Assignment 1 FDS/data_formatted.pkl')\n",
    "data_formatted = pd.read_pickle(\"/content/drive/My Drive/Colab Notebooks/Assignment 1 FDS/data_formatted.pkl\")\n",
    "data_formatted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "itiP-z2vDQYf"
   },
   "source": [
    "# 2) Cleaning, Filtering and Formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BcLwiDKxDh7h"
   },
   "source": [
    "## 2.1) Only US, English Languague"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8F9uT4QzDpZI"
   },
   "outputs": [],
   "source": [
    "#Exclude tweets outside of US (location) - filter on country code US (Country where tweet is posted)\n",
    "data_formatted = data_formatted[data_formatted['place_country_code'] == \"US\"]\n",
    "data_formatted = data_formatted[data_formatted['tweet_lang'] == 'en']\n",
    "len(data_formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1cqnmPARDVY6"
   },
   "source": [
    "## 2.2) State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lxkfz7kozo_8"
   },
   "outputs": [],
   "source": [
    "data_formatted['coordinates'] = data_formatted['place_longitude'].astype(str) + \", \" + data_formatted['place_latitude'].astype(str)\n",
    "data_formatted['state'] = data_formatted['coordinates'].apply(city_state_country)\n",
    "#data_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m_ojohFlzOoL"
   },
   "outputs": [],
   "source": [
    "data_formatted.to_pickle('/content/drive/My Drive/Colab Notebooks/Assignment 1 FDS/data_formatted.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aRBZd3G2ceqF"
   },
   "source": [
    "## 2.3) Hash/Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "58QtcdV1cjoe"
   },
   "outputs": [],
   "source": [
    "#Determine tags and hashs (some tweets dont have tags only hash)\n",
    "data_formatted['tags_mention'] = data_formatted.text.str.findall(r'(?<![@\\w])@(\\w{1,25})').apply(','.join)\n",
    "data_formatted['hash_mention'] = data_formatted.text.str.findall(r\"#(\\w+)\").apply(','.join)\n",
    "data_formatted['tags_hash'] = data_formatted['hash_mention'] + data_formatted['tags_mention']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "in4-E0Uqg2np"
   },
   "outputs": [],
   "source": [
    "#Lower cases in tags and hashes\n",
    "data_formatted['tags_hash'] = data_formatted['tags_hash'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TuLATpHNg4ti"
   },
   "outputs": [],
   "source": [
    "#See if either trum or clinton mentioned in hash or tags\n",
    "data_formatted['Trump'] = data_formatted['tags_hash'].str.contains(r'trump|maga', na=False)\n",
    "data_formatted['Clinton'] = data_formatted['tags_hash'].str.contains(r'clinton|imwithher|hillary', na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ChCcowe-zQVW"
   },
   "outputs": [],
   "source": [
    "data_formatted.to_pickle('/content/drive/My Drive/Colab Notebooks/Assignment 1 FDS/data_formatted.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "otJPCe1yG_jv"
   },
   "source": [
    "## 2.4) Filtering Bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bfn4aE57Grqm"
   },
   "outputs": [],
   "source": [
    "#Filter bots (hyperlinks in text)\n",
    "#Removing dubious sources: \n",
    "sources = ('<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>','<a href=\"http://twitter.com/download/android\" rel=\"nofollow\">Twitter for Android</a>',\n",
    "                '<a href=\"http://twitter.com\" rel=\"nofollow\">Twitter Web Client</a>', '<a href=\"https://twitter.com/download/android\" rel=\"nofollow\">Twitter for Android Tablets</a>',\n",
    "                '<a href=\"http://twitter.com/#!/download/ipad\" rel=\"nofollow\">Twitter for iPad</a>','<a href=\"http://instagram.com\" rel=\"nofollow\">Instagram</a>')\n",
    "\n",
    "data_formatted =data_formatted[data_formatted.source.isin(sources)]\n",
    "\n",
    "#Removing people with no friends and no followers \n",
    "data_formatted = data_formatted[(data_formatted.followers_count > 0) & (data_formatted.followers_count > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z4eKgpcosBYf"
   },
   "outputs": [],
   "source": [
    "data_formatted.to_pickle('/content/drive/My Drive/Colab Notebooks/Assignment 1 FDS/data_formatted.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BerrU9uVD_yd"
   },
   "source": [
    "## 2.5) More Columns - Same as 2.3??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wud8-o6lD9mM"
   },
   "outputs": [],
   "source": [
    "#Determine tags and hashs (some tweets dont have tags only hash)\n",
    "data_formatted['tags_mention'] = data_formatted.text.str.findall(r'(?<![@\\w])@(\\w{1,25})').apply(','.join)\n",
    "data_formatted['hash_mention'] = data_formatted.text.str.findall(r\"#(\\w+)\").apply(','.join)\n",
    "data_formatted['tags_hash'] = data_formatted['hash_mention'] + data_formatted['tags_mention']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aa7_B0tdEHCP"
   },
   "outputs": [],
   "source": [
    "#Lower cases in tags and hashes\n",
    "data_formatted['tags_hash'] = data_formatted['tags_hash'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7kL1TCGeEKS6"
   },
   "outputs": [],
   "source": [
    "#See if either trum or clinton mentioned in hash or tags\n",
    "data_formatted['Trump'] = data_formatted['tags_hash'].str.contains(r'trump', na=False)\n",
    "data_formatted['Clinton'] = data_formatted['tags_hash'].str.contains(r'clinton', na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fVBBQrFUsUuN"
   },
   "outputs": [],
   "source": [
    "data_formatted.to_pickle('/content/drive/My Drive/Colab Notebooks/Assignment 1 FDS/data_formatted.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1u8rQsLUERkL"
   },
   "source": [
    "# 3) Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3-A7RqE3EQDx"
   },
   "outputs": [],
   "source": [
    "#data_(\"How many different ids we have : \" + str(data_formatted3['id'].nunique()))\n",
    "#print(\"How many different ids we have : \" + str(data_formatted['location'].unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XKsazjA7Z140"
   },
   "source": [
    "# 4) Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rys1DpqbZ141"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C8tHU-A9Z142"
   },
   "outputs": [],
   "source": [
    "#Loading training dataset \n",
    "training_set = pd.read_csv(\"trainingandtestdata/training.1600000.processed.noemoticon.csv\", \n",
    "                 encoding = 'latin-1',\n",
    "                 names = [\"Polarity\", \"Tweet ID\",\"Date\",\"Query\",\"User\",\"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mUWx70VVZ144"
   },
   "outputs": [],
   "source": [
    "#Define english stopwords\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pvOpgyHEZ146"
   },
   "outputs": [],
   "source": [
    "#Filter out Polarity Score equal 0 or 4\n",
    "training_set = training_set[(training_set[\"Polarity\"] == 4) | (training_set[\"Polarity\"] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ALX0aqtIZ147"
   },
   "outputs": [],
   "source": [
    "#Data cleaning - removing punctuation from text; removing tags; removing links\n",
    "training_set['Text'] = training_set['Text'].str.replace('@[^\\s]+','')\n",
    "training_set['Text'] = training_set['Text'].str.replace('[^\\w\\s]','')\n",
    "training_set['Text'] = training_set['Text'].str.replace('\\[.*?\\]','')\n",
    "training_set['Text'] = training_set['Text'].str.replace('[‘’“”…]','')\n",
    "training_set['Text'] = training_set['Text'].str.replace('\\w*\\d\\w*','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UXXoS6SsZ148"
   },
   "outputs": [],
   "source": [
    "#Create data that puts tweet texts together with sentiment score\n",
    "tweets_and_polarity_scores = []\n",
    "for i in range(0, len(training_set)):\n",
    "    if training_set[\"Polarity\"].iloc[i] == 0 or training_set[\"Polarity\"].iloc[i] == 4: \n",
    "        tweet_text = training_set['Text'].iloc[i]\n",
    "        tweet_words_in_list = [e.lower() for e in tweet_text.split()]\n",
    "        tweet_words_in_list_wo_stopwords = [e for e in tweet_words_in_list if not e in stopwords]\n",
    "        tweet_polarity_score = training_set[\"Polarity\"].iloc[i]\n",
    "        tweets_and_polarity_scores.append((tweet_words_in_list_wo_stopwords, tweet_polarity_score))\n",
    "    else: \n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vuuuOkzpZ14-"
   },
   "outputs": [],
   "source": [
    "#Select random sample - otherwise the classifier will be too slow\n",
    "random_sample = random.sample(tweets_and_polarity_scores, k = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hZnnsqiiZ14_"
   },
   "outputs": [],
   "source": [
    "def get_words_in_tweets(tweets):\n",
    "    all_words= []\n",
    "    for (words, sentiment) in tweets:\n",
    "        all_words.extend(words)\n",
    "    return all_words\n",
    "\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = list(wordlist.keys())\n",
    "    return word_features\n",
    "\n",
    "word_features = get_word_features(get_words_in_tweets(random_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OQk0JlqNZ15B"
   },
   "outputs": [],
   "source": [
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[\"contains(%s)\" % word] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jnx7Voj9Z15C"
   },
   "outputs": [],
   "source": [
    "final_training_set = nltk.classify.apply_features(extract_features, random_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LM48t38lZ15D"
   },
   "outputs": [],
   "source": [
    "#Classifier way too slow atm\n",
    "classifier = nltk.NaiveBayesClassifier.train(final_training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-VmJa_0mZ15E",
    "outputId": "bbe6ba86-a825-444e-ff29-3416580dc336"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 689,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing\n",
    "tweet_positive = 'I like Larry'\n",
    "classifier.classify(extract_features(tweet_positive.split()))\n",
    "#classifier.show_most_informative_features(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uwwF-FDCZ15F"
   },
   "outputs": [],
   "source": [
    "#Loading test dataset \n",
    "test_set = pd.read_csv(\"trainingandtestdata/testdata.manual.2009.06.14.csv\", \n",
    "                 encoding = 'latin-1',\n",
    "                 names = [\"Polarity\", \"Tweet ID\",\"Date\",\"Query\",\"User\",\"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fh8KchxaZ15G"
   },
   "outputs": [],
   "source": [
    "#Data cleaning - removing punctuation from text\n",
    "test_set['Text'] = test_set['Text'].str.replace('@[^\\s]+','')\n",
    "test_set['Text'] = test_set['Text'].str.replace('[^\\w\\s]','')\n",
    "test_set['Text'] = test_set['Text'].str.replace('\\[.*?\\]','')\n",
    "test_set['Text'] = test_set['Text'].str.replace('[‘’“”…]','')\n",
    "test_set['Text'] = test_set['Text'].str.replace('\\w*\\d\\w*','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vjDAJ4RzZ15H"
   },
   "outputs": [],
   "source": [
    "#Split words in tweets; remove stop words\n",
    "test_set['Text'] = test_set['Text'].apply(lambda x: x.lower())\n",
    "test_set['Text'] = test_set['Text'].apply(lambda x: x.split())\n",
    "test_set['Text'] = test_set['Text'].apply(lambda x: [item for item in x if item not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yWJ1vqBXZ15I"
   },
   "outputs": [],
   "source": [
    "#Apply classifier to column \n",
    "test_set['classification_polarity'] = test_set['Text'].apply(lambda x: classifier.classify(extract_features(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SbyGSJcPZ15J",
    "outputId": "945289da-df4c-4ac2-8b16-4e6ba11c37ba"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Tweet ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Query</th>\n",
       "      <th>User</th>\n",
       "      <th>Text</th>\n",
       "      <th>classification_polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Mon May 11 03:17:40 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>tpryan</td>\n",
       "      <td>[loooooooovvvvvveee, dx, cool, fantastic, right]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:18:03 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>vcu451</td>\n",
       "      <td>[reading, love, lee, childs, good, read]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Mon May 11 03:18:54 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>chadfu</td>\n",
       "      <td>[ok, first, assesment, fucking, rocks]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>Mon May 11 03:19:04 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>SIX15</td>\n",
       "      <td>[youll, love, ive, mine, months, never, looked...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>Mon May 11 03:21:41 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>yamarama</td>\n",
       "      <td>[fair, enough, think, perfect]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Polarity  Tweet ID                          Date    Query      User  \\\n",
       "0         4         3  Mon May 11 03:17:40 UTC 2009  kindle2    tpryan   \n",
       "1         4         4  Mon May 11 03:18:03 UTC 2009  kindle2    vcu451   \n",
       "2         4         5  Mon May 11 03:18:54 UTC 2009  kindle2    chadfu   \n",
       "3         4         6  Mon May 11 03:19:04 UTC 2009  kindle2     SIX15   \n",
       "4         4         7  Mon May 11 03:21:41 UTC 2009  kindle2  yamarama   \n",
       "\n",
       "                                                Text  classification_polarity  \n",
       "0   [loooooooovvvvvveee, dx, cool, fantastic, right]                        4  \n",
       "1           [reading, love, lee, childs, good, read]                        4  \n",
       "2             [ok, first, assesment, fucking, rocks]                        4  \n",
       "3  [youll, love, ive, mine, months, never, looked...                        0  \n",
       "4                     [fair, enough, think, perfect]                        4  "
      ]
     },
     "execution_count": 865,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ie4IlrbYZ15K"
   },
   "source": [
    "# 5) Topic Modelling - LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SmGhMz0zhBrh"
   },
   "source": [
    "## Using all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s3foqGOGikfO"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KH1JrEp1Z15K"
   },
   "outputs": [],
   "source": [
    "data_sentimentanalysis = data_formatted.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LAVJI_6HiqRs"
   },
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(w) for w in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tEOWpNlOivjd"
   },
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G24yf6JdiyW3"
   },
   "outputs": [],
   "source": [
    "#Data cleaning - removing punctuation from text; still need to remove hashes\n",
    "data_sentimentanalysis['text'] = data_sentimentanalysis['text'].str.replace('@[^\\s]+','')\n",
    "data_sentimentanalysis['text'] = data_sentimentanalysis['text'].str.replace('[^\\w\\s]','')\n",
    "data_sentimentanalysis['text'] = data_sentimentanalysis['text'].str.replace('\\[.*?\\]','')\n",
    "data_sentimentanalysis['text'] = data_sentimentanalysis['text'].str.replace('[‘’“”…]','')\n",
    "data_sentimentanalysis['text'] = data_sentimentanalysis['text'].str.replace('\\w*\\d\\w*','')\n",
    "data_sentimentanalysis['text'] = data_sentimentanalysis['text'].apply(lambda x: x.lower())\n",
    "data_sentimentanalysis['text'] = data_sentimentanalysis['text'].apply(lambda x: x.split())\n",
    "data_sentimentanalysis['text'] = data_sentimentanalysis['text'].apply(lambda x: [item for item in x if item not in stopwords])\n",
    "data_sentimentanalysis['text'] = data_sentimentanalysis['text'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vygwfUTMi0na"
   },
   "outputs": [],
   "source": [
    "#Create data set for trump vs clinton\n",
    "trump_mentioned_true = data_sentimentanalysis[data_sentimentanalysis['Trump']]\n",
    "clinton_mentioned_true = data_sentimentanalysis[data_sentimentanalysis['Clinton']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0aNs7EBai3DF"
   },
   "outputs": [],
   "source": [
    "#Filter out rows where text doesnt contain any entries after removing tags and hashes\n",
    "trump_mentioned_true = trump_mentioned_true[trump_mentioned_true['text'].map(lambda d: len(d)) > 0]\n",
    "trump_text = trump_mentioned_true.text.tolist()\n",
    "dictionary_trump = corpora.Dictionary(trump_text)\n",
    "doc_term_matrix_trump = [dictionary_trump.doc2bow(doc) for doc in trump_text]\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda_trump = gensim.models.ldamodel.LdaModel\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel_trump = Lda_trump(doc_term_matrix_trump, num_topics=2, id2word = dictionary_trump, passes=100)\n",
    "topics_trump = ldamodel_trump.print_topics(num_topics=2, num_words=8)\n",
    "topics_trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sk6jlSC9i5Cn"
   },
   "outputs": [],
   "source": [
    "#Filter out rows where text doesnt contain any entries after removing tags and hashes\n",
    "clinton_mentioned_true = clinton_mentioned_true[clinton_mentioned_true['text'].map(lambda d: len(d)) > 0]\n",
    "clinton_text = clinton_mentioned_true.text.tolist()\n",
    "dictionary_clinton = corpora.Dictionary(clinton_text)\n",
    "doc_term_matrix_clinton = [dictionary_clinton.doc2bow(doc) for doc in clinton_text]\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda_clinton = gensim.models.ldamodel.LdaModel\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel_clinton = Lda_clinton(doc_term_matrix_clinton, num_topics=2, id2word = dictionary_clinton, passes=100)\n",
    "topics_clinton = ldamodel_clinton.print_topics(num_topics=10, num_words=4)\n",
    "topics_clinton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KTZUL9TFi9U_"
   },
   "source": [
    "## Only using nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wYkIP0iEi_6Q"
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mqUZE621jCKc"
   },
   "outputs": [],
   "source": [
    "#Apply function that determines the type of each word\n",
    "trump_mentioned_text = trump_mentioned_true['text'].apply(lambda x: nltk.pos_tag(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7TFf2O7rjEbT"
   },
   "outputs": [],
   "source": [
    "#Filter out the nouns for each row\n",
    "trump_mentioned_nouns = pd.DataFrame(np.zeros((len(trump_mentioned_true), 1)))\n",
    "trump_mentioned_nouns.columns = ['Text']\n",
    "for i in range(0, len(trump_mentioned_text)):\n",
    "    nouns = [word for (word, pos) in trump_mentioned_text.iloc[i] if is_noun(pos)]\n",
    "    nouns = ' '.join(nouns)\n",
    "    trump_mentioned_nouns.iloc[i] = nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tsXehVGhjGIp"
   },
   "outputs": [],
   "source": [
    "#Split the Text \n",
    "trump_mentioned_nouns['Text'] = trump_mentioned_nouns['Text'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XvvffzW-jIpn"
   },
   "outputs": [],
   "source": [
    "trump_text = trump_mentioned_nouns.Text.tolist()\n",
    "dictionary_trump = corpora.Dictionary(trump_text)\n",
    "doc_term_matrix_trump = [dictionary_trump.doc2bow(doc) for doc in trump_text]\n",
    "Lda_trump = gensim.models.ldamodel.LdaModel\n",
    "ldamodel_trump = Lda_trump(doc_term_matrix_trump, num_topics=10, id2word = dictionary_trump, passes=100)\n",
    "topics_trump = ldamodel_trump.print_topics(num_topics=10, num_words=8)\n",
    "topics_trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "byaFHLJYjKPq"
   },
   "outputs": [],
   "source": [
    "#Apply function that determines the type of each word\n",
    "clinton_mentioned_text = trump_mentioned_true['text'].apply(lambda x: nltk.pos_tag(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tzvTHBkNjMeL"
   },
   "outputs": [],
   "source": [
    "#Filter out the nouns for each row\n",
    "clinton_mentioned_nouns = pd.DataFrame(np.zeros((len(clinton_mentioned_true), 1)))\n",
    "clinton_mentioned_nouns.columns = ['Text']\n",
    "for i in range(0, len(clinton_mentioned_text)):\n",
    "    nouns = [word for (word, pos) in clinton_mentioned_nouns.iloc[i] if is_noun(pos)]\n",
    "    nouns = ' '.join(nouns)\n",
    "    clinton_mentioned_nouns.iloc[i] = nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lSdy9P_-jTbE"
   },
   "outputs": [],
   "source": [
    "#Split the text \n",
    "clinton_mentioned_nouns['Text'] = clinton_mentioned_nouns['Text'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nI9k24mCjT5s"
   },
   "outputs": [],
   "source": [
    "clinton_text = clinton_mentioned_nouns.text.tolist()\n",
    "dictionary_clinton = corpora.Dictionary(clinton_text)\n",
    "doc_term_matrix_clinton = [dictionary_clinton.doc2bow(doc) for doc in clinton_text]\n",
    "Lda_clinton = gensim.models.ldamodel.LdaModel\n",
    "ldamodel_clinton = Lda_clinton(doc_term_matrix_clinton, num_topics=10, id2word = dictionary_clinton, passes=100)\n",
    "topics_clinton = ldamodel_clinton.print_topics(num_topics=10, num_words=8)\n",
    "topics_clinton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pLbIQ_OStNV0"
   },
   "source": [
    "# Others (No need to run this)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dr48Yn5-Z14q"
   },
   "outputs": [],
   "source": [
    "#Excluding tweets referring to Clinton & Trump\n",
    "#Getting sentiment score on daily basis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "31URZIH_-TvZ"
   },
   "source": [
    "Mapping City to State DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "jApICWJei0jd",
    "outputId": "b02b7110-48e8-45d2-e817-1129da2795a1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME</th>\n",
       "      <th>STNAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abbeville</td>\n",
       "      <td>Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adamsville</td>\n",
       "      <td>Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Alabaster</td>\n",
       "      <td>Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Albertville</td>\n",
       "      <td>Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Alexander City</td>\n",
       "      <td>Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81240</th>\n",
       "      <td>Lander</td>\n",
       "      <td>Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81241</th>\n",
       "      <td>Laramie</td>\n",
       "      <td>Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81264</th>\n",
       "      <td>Rawlins</td>\n",
       "      <td>Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81268</th>\n",
       "      <td>Rock Springs</td>\n",
       "      <td>Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81285</th>\n",
       "      <td>Worland</td>\n",
       "      <td>Wyoming</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8234 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 NAME   STNAME\n",
       "1           Abbeville  Alabama\n",
       "2          Adamsville  Alabama\n",
       "5           Alabaster  Alabama\n",
       "6         Albertville  Alabama\n",
       "7      Alexander City  Alabama\n",
       "...               ...      ...\n",
       "81240          Lander  Wyoming\n",
       "81241         Laramie  Wyoming\n",
       "81264         Rawlins  Wyoming\n",
       "81268    Rock Springs  Wyoming\n",
       "81285         Worland  Wyoming\n",
       "\n",
       "[8234 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading File\n",
    "locations = pd.read_csv(\"https://www2.census.gov/programs-surveys/popest/datasets/2010-2019/cities/totals/sub-est2019_all.csv\", encoding = 'latin-1')\n",
    "\n",
    "#Keep Columns we need\n",
    "locations = locations[['NAME','STNAME']].copy()\n",
    "\n",
    "#Only cities\n",
    "locations = locations[locations['NAME'].str.contains(\"city\")]\n",
    "\n",
    "#Remove \"city\" word\n",
    "locations[\"NAME\"] = locations[\"NAME\"].str.replace(\"city\", \"\")\n",
    "#locations[\"NAME\"] = locations[\"NAME\"].str.replace(\"town\", \"\")\n",
    "#locations[\"NAME\"] = locations[\"NAME\"].str.replace(\"County\", \"\")\n",
    "#locations[\"NAME\"] = locations[\"NAME\"].str.replace(\"Balance of \", \"\")\n",
    "\n",
    "#trimming\n",
    "locations[\"NAME\"] = locations[\"NAME\"].str.strip()\n",
    "\n",
    "#Unique Cities\n",
    "locations.drop_duplicates(subset = 'NAME', keep = 'first', inplace = True)\n",
    "locations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0dtSt3B2_8PG"
   },
   "outputs": [],
   "source": [
    "def city_to_state(city):\n",
    "    \"\"\"\n",
    "    Gets the state of a city\n",
    "    :param city: the city name for which we are looking the state\n",
    "    :return data:\n",
    "    \"\"\"\n",
    "    state = locations[locations['NAME'].str.lower().str.contains(city.lower())]['STNAME'].head(1)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "s6f8tE_BF5yA",
    "outputId": "bc95165a-34eb-417c-d253-983a28fb68ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33132    Minnesota\n",
       "Name: STNAME, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_to_state(\"New York\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "fEHhvoQQDZQY",
    "outputId": "101f45b3-7fb2-43f0-8951-32675118f373"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5278    Florida\n",
       "Name: STNAME, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_to_state(data_formatted['location'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W5ShLyOZZ14k"
   },
   "outputs": [],
   "source": [
    "#Change dictionary into dataframe\n",
    "#Load dictionaries within dictionaries (user, place) separately. From user exclude unnecessary columns\n",
    "data_formatted = []\n",
    "for i in range(0, 1000): \n",
    "    data_f = json.loads(data[i])\n",
    "    user = data_f['user']\n",
    "    user = pd.DataFrame([user])\n",
    "    unwanted_user = set(['id_str', 'profile_background_color', 'profile_background_image_url', \n",
    "                         'profile_background_image_url_https', 'profile_background_tile', \n",
    "                        'profile_link_color', 'profile_sidebar_border_color', \n",
    "                        'profile_sidebar_fill_color', 'profile_text_color', 'profile_use_background_image', \n",
    "                        'profile_image_url', 'profile_image_url_https', 'default_profile_image']) \n",
    "    for unwanted_key in unwanted_user: del user[unwanted_key]\n",
    "    place = data_f['place']\n",
    "    place = pd.DataFrame([place])\n",
    "    \n",
    "    place = place[['full_name', 'country_code', 'bounding_box']]\n",
    "    latitude = data_f['place']['bounding_box']['coordinates'][0][0][0]\n",
    "    longitude = data_f['place']['bounding_box']['coordinates'][0][0][1]\n",
    "    \n",
    "    unwanted = set(['user', 'place', 'entities']) #Dictionaries within dictionaries\n",
    "    for unwanted_key in unwanted: del data_f[unwanted_key]\n",
    "    data_pre = pd.DataFrame([data_f])\n",
    "    data_pre['latitude'] = latitude;\n",
    "    data_pre['longitude'] = longitude;\n",
    "    user = user.rename({'id':'user_id'}, axis=1)\n",
    "    if i == 0: \n",
    "        data_formatted = data_pre\n",
    "        user_formatted = user\n",
    "        place_formatted = place\n",
    "    else:\n",
    "        data_formatted = data_formatted.append(data_pre, ignore_index=True)\n",
    "        user_formatted = user_formatted.append(user, ignore_index=True)\n",
    "        place_formatted = place_formatted.append(place, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "colab_type": "code",
    "id": "PkI_O0jKZ14n",
    "outputId": "8b500e6c-cd43-44a5-ec3e-39977f6bc734"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-189-e0c71916d75b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Merge data frame (without dictionaries within dictionaries), user and place dictionary together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_formatted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_formatted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_formatted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplace_formatted\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata_formatted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             new_data = concatenate_block_managers(\n\u001b[0;32m--> 497\u001b[0;31m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m             )\n\u001b[1;32m    499\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m   2025\u001b[0m         \u001b[0mblocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2027\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, blocks, axes, do_integrity_check)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_integrity_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verify_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_verify_integrity\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verify_integrity\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mmgr_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m                 \u001b[0mconstruction_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtot_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtot_items\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             raise AssertionError(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mconstruction_error\u001b[0;34m(tot_items, block_shape, axes, e)\u001b[0m\n\u001b[1;32m   1692\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mblock_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Empty data passed with indices specified.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1694\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Shape of passed values is {passed}, indices imply {implied}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (1000999, 53), indices imply (1999, 53)"
     ]
    }
   ],
   "source": [
    "#Merge data frame (without dictionaries within dictionaries), user and place dictionary together\n",
    "#data_formatted = pd.concat([data_formatted, user_formatted, place_formatted], axis=1)\n",
    "#data_formatted.head()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment1_FDS_2009.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
